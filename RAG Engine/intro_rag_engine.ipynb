{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Intro to Building a Scalable and Modular RAG System with RAG Engine in Vertex AI\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Frag-engine%2Fintro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84f0f73a0f76"
      },
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Retrieval Augmented Generation (RAG) improves Large Language Models (LLMs) by allowing them to access and process external information sources during generation. This ensures the model's responses are grounded in factual data and avoids hallucinations.\n",
        "\n",
        "A common problem with LLMs is that they don't understand private knowledge, that\n",
        "is, your organization's data. With RAG Engine, you can enrich the\n",
        "LLM context with additional private information, because the model can reduce\n",
        "hallucinations and answer questions more accurately.\n",
        "\n",
        "By combining additional knowledge sources with the existing knowledge that LLMs\n",
        "have, a better context is provided. The improved context along with the query\n",
        "enhances the quality of the LLM's response.\n",
        "\n",
        "The following concepts are key to understanding Vertex AI RAG Engine. These concepts are listed in the order of the\n",
        "retrieval-augmented generation (RAG) process.\n",
        "\n",
        "1. **Data ingestion**: Intake data from different data sources. For example,\n",
        "  local files, Google Cloud Storage, and Google Drive.\n",
        "\n",
        "1. **Data transformation**: Conversion of the data in preparation for indexing. For example, data is split into chunks.\n",
        "\n",
        "1. **Embedding**: Numerical representations of words or pieces of text. These numbers capture the\n",
        "   semantic meaning and context of the text. Similar or related words or text\n",
        "   tend to have similar embeddings, which means they are closer together in the\n",
        "   high-dimensional vector space.\n",
        "\n",
        "1. **Data indexing**: RAG Engine creates an index called a corpus.\n",
        "   The index structures the knowledge base so it's optimized for searching. For\n",
        "   example, the index is like a detailed table of contents for a massive\n",
        "   reference book.\n",
        "\n",
        "1. **Retrieval**: When a user asks a question or provides a prompt, the retrieval\n",
        "  component in RAG Engine searches through its knowledge\n",
        "  base to find information that is relevant to the query.\n",
        "\n",
        "1. **Generation**: The retrieved information becomes the context added to the\n",
        "  original user query as a guide for the generative AI model to generate\n",
        "  factually grounded and relevant responses.\n",
        "\n",
        "For more information, refer to the public documentation for [Vertex AI RAG Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61RBz8LLbxCR"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK and Google Gen AI SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db188f6-0059-4e7e-cbaa-124bebc3b41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NyKGtVQjgx13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38368360-e364-40f5-8e4f-3c0ad934b7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "See [supported regions](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview#supported-regions) for location options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "# Use the environment variable if the user doesn't provide Project ID.\n",
        "import os\n",
        "\n",
        "import vertexai\n",
        "from google import genai\n",
        "\n",
        "# fmt: off\n",
        "PROJECT_ID = \"project-xxxxxxxxxxxxxx\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "# fmt: on\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "# See https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview#supported-regions for location options.\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-east1\")\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=\"global\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5303c05f7aa6"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6fc324893334"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google.genai.types import GenerateContentConfig, Retrieval, Tool, VertexRagStore\n",
        "from vertexai import rag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e43229f3ad4f"
      },
      "source": [
        "### Create a RAG Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cf93d5f0ce00"
      },
      "outputs": [],
      "source": [
        "# Currently supports Google first-party embedding models\n",
        "# fmt: off\n",
        "EMBEDDING_MODEL = \"publishers/google/models/text-embedding-005\"  # @param {type:\"string\", isTemplate: true}\n",
        "# fmt: on\n",
        "\n",
        "rag_corpus = rag.create_corpus(\n",
        "    display_name=\"my-rag-corpus\",\n",
        "    backend_config=rag.RagVectorDbConfig(\n",
        "        rag_embedding_model_config=rag.RagEmbeddingModelConfig(\n",
        "            vertex_prediction_endpoint=rag.VertexPredictionEndpoint(\n",
        "                publisher_model=EMBEDDING_MODEL\n",
        "            )\n",
        "        )\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "197c585b61b2"
      },
      "source": [
        "### Check the corpus just created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f229b13dc617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9faae5f5-76b8-4cde-fed1-f37d1aeda65e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ListRagCorporaPager<rag_corpora {\n",
              "  name: \"projects/project-ee7cb7bb-2d31-45e1-83d/locations/us-east1/ragCorpora/4611686018427387904\"\n",
              "  display_name: \"my-rag-corpus\"\n",
              "  create_time {\n",
              "    seconds: 1769164957\n",
              "    nanos: 100672000\n",
              "  }\n",
              "  update_time {\n",
              "    seconds: 1769164957\n",
              "    nanos: 100672000\n",
              "  }\n",
              "  corpus_status {\n",
              "    state: ACTIVE\n",
              "  }\n",
              "  vector_db_config {\n",
              "    rag_managed_db {\n",
              "      knn {\n",
              "      }\n",
              "    }\n",
              "    rag_embedding_model_config {\n",
              "      vertex_prediction_endpoint {\n",
              "        endpoint: \"projects/project-ee7cb7bb-2d31-45e1-83d/locations/us-east1/publishers/google/models/text-embedding-005\"\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              ">"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "rag.list_corpora()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52924cc1440"
      },
      "source": [
        "### Upload a local file to the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4976ffe8564f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3b46b4-d6c5-4600-b3e2-4c840b09335d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.md\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.md\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here's a breakdown:\n",
        "\n",
        "**What it is:**\n",
        "\n",
        "* **Combining Retrieval and Generation:**\n",
        "    * RAG combines the strengths of information retrieval systems (like search engines) with the generative power of LLMs.\n",
        "    * It enables LLMs to go beyond their pre-trained data and access up-to-date and specific information.\n",
        "* **How it works:**\n",
        "    * When a user asks a question, the RAG system first retrieves relevant information from external data sources (e.g., databases, documents, web pages).\n",
        "    * This retrieved information is then provided to the LLM as additional context.\n",
        "    * The LLM uses this augmented context to generate a more accurate and informative response.\n",
        "\n",
        "**Why it's helpful:**\n",
        "\n",
        "* **Access to Up-to-Date Information:**\n",
        "    * LLMs are trained on static datasets, so their knowledge can become outdated. RAG allows them to access real-time or frequently updated information.\n",
        "* **Improved Accuracy and Factual Grounding:**\n",
        "    * RAG reduces the risk of LLM \"hallucinations\" (generating false or misleading information) by grounding responses in verified external data.\n",
        "* **Enhanced Contextual Relevance:**\n",
        "    * By providing relevant context, RAG enables LLMs to generate more precise and tailored responses to specific queries.\n",
        "* **Increased Trust and Transparency:**\n",
        "    * RAG can provide source citations, allowing users to verify the information and increasing trust in the LLM's responses.\n",
        "* **Cost Efficiency:**\n",
        "    * Rather than constantly retraining large language models, RAG allows for the introduction of new data in a more cost effective way.\n",
        "\n",
        "In essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "529390917c29"
      },
      "outputs": [],
      "source": [
        "rag_file = rag.upload_file(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    path=\"test.md\",\n",
        "    display_name=\"test.md\",\n",
        "    description=\"my test file\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5269a0c2786d"
      },
      "source": [
        "### Import files from Google Cloud Storage\n",
        "\n",
        "Remember to grant \"Viewer\" access to the \"Vertex RAG Data Service Agent\" (with the format of `service-{project_number}@gcp-sa-vertex-rag.iam.gserviceaccount.com`) for your Google Cloud Storage bucket.\n",
        "\n",
        "For this example, we'll use a public GCS bucket containing earning reports from Alphabet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5910ae450f69"
      },
      "outputs": [],
      "source": [
        "INPUT_GCS_BUCKET = (\n",
        "    \"gs://cloud-samples-data/gen-app-builder/search/alphabet-investor-pdfs/\"\n",
        ")\n",
        "\n",
        "response = rag.import_files(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    paths=[INPUT_GCS_BUCKET],\n",
        "    # Optional\n",
        "    transformation_config=rag.TransformationConfig(\n",
        "        chunking_config=rag.ChunkingConfig(chunk_size=1024, chunk_overlap=100)\n",
        "    ),\n",
        "    max_embedding_requests_per_min=900,  # Optional\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60a84095746d"
      },
      "source": [
        "### Import files from Google Drive\n",
        "\n",
        "Eligible paths can be formatted as:\n",
        "\n",
        "- `https://drive.google.com/drive/folders/{folder_id}`\n",
        "- `https://drive.google.com/file/d/{file_id}`.\n",
        "\n",
        "Remember to grant \"Viewer\" access to the \"Vertex RAG Data Service Agent\" (with the format of `service-{project_number}@gcp-sa-vertex-rag.iam.gserviceaccount.com`) for your Drive folder/files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a90c125874c"
      },
      "outputs": [],
      "source": [
        "response = rag.import_files(\n",
        "    corpus_name=rag_corpus.name,\n",
        "    paths=[\"https://drive.google.com/drive/folders/{folder_id}\"],\n",
        "    # Optional\n",
        "    transformation_config=rag.TransformationConfig(\n",
        "        chunking_config=rag.ChunkingConfig(chunk_size=512, chunk_overlap=50)\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f700b3e23121"
      },
      "source": [
        "### Optional: Perform direct context retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4669c5cdbb5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ccfd0c-3917-45e7-882b-862e35bf9109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contexts {\n",
            "  contexts {\n",
            "    source_uri: \"test.md\"\n",
            "    text: \"Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here\\'s a breakdown:\\n**What it is:**\\n* **Combining Retrieval and Generation:**\\n    * RAG combines the strengths of information retrieval systems (like search engines) with the generative power of LLMs.\\n    * It enables LLMs to go beyond their pre-trained data and access up-to-date and specific information.\\n* **How it works:**\\n    * When a user asks a question, the RAG system first retrieves relevant information from external data sources (e.g., databases, documents, web pages).\\n    * This retrieved information is then provided to the LLM as additional context.\\n    * The LLM uses this augmented context to generate a more accurate and informative response.\\n**Why it\\'s helpful:**\\n* **Access to Up-to-Date Information:**\\n    * LLMs are trained on static datasets, so their knowledge can become outdated. RAG allows them to access real-time or frequently updated information.\\n* **Improved Accuracy and Factual Grounding:**\\n    * RAG reduces the risk of LLM \\\"hallucinations\\\" (generating false or misleading information) by grounding responses in verified external data.\\n* **Enhanced Contextual Relevance:**\\n    * By providing relevant context, RAG enables LLMs to generate more precise and tailored responses to specific queries.\\n* **Increased Trust and Transparency:**\\n    * RAG can provide source citations, allowing users to verify the information and increasing trust in the LLM\\'s responses.\\n* **Cost Efficiency:**\\n    * Rather than constantly retraining large language models, RAG allows for the introduction of new data in a more cost effective way.\\nIn essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.\"\n",
            "    source_display_name: \"test.md\"\n",
            "    score: 0.24961314293135028\n",
            "    chunk {\n",
            "      text: \"Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by allowing them to access and incorporate external data sources when generating responses. Here\\'s a breakdown:\\n**What it is:**\\n* **Combining Retrieval and Generation:**\\n    * RAG combines the strengths of information retrieval systems (like search engines) with the generative power of LLMs.\\n    * It enables LLMs to go beyond their pre-trained data and access up-to-date and specific information.\\n* **How it works:**\\n    * When a user asks a question, the RAG system first retrieves relevant information from external data sources (e.g., databases, documents, web pages).\\n    * This retrieved information is then provided to the LLM as additional context.\\n    * The LLM uses this augmented context to generate a more accurate and informative response.\\n**Why it\\'s helpful:**\\n* **Access to Up-to-Date Information:**\\n    * LLMs are trained on static datasets, so their knowledge can become outdated. RAG allows them to access real-time or frequently updated information.\\n* **Improved Accuracy and Factual Grounding:**\\n    * RAG reduces the risk of LLM \\\"hallucinations\\\" (generating false or misleading information) by grounding responses in verified external data.\\n* **Enhanced Contextual Relevance:**\\n    * By providing relevant context, RAG enables LLMs to generate more precise and tailored responses to specific queries.\\n* **Increased Trust and Transparency:**\\n    * RAG can provide source citations, allowing users to verify the information and increasing trust in the LLM\\'s responses.\\n* **Cost Efficiency:**\\n    * Rather than constantly retraining large language models, RAG allows for the introduction of new data in a more cost effective way.\\nIn essence, RAG bridges the gap between the vast knowledge of LLMs and the need for accurate, current, and contextually relevant information.\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Direct context retrieval\n",
        "response = rag.retrieval_query(\n",
        "    rag_resources=[\n",
        "        rag.RagResource(\n",
        "            rag_corpus=rag_corpus.name,\n",
        "            # Optional: supply IDs from `rag.list_files()`.\n",
        "            # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
        "        )\n",
        "    ],\n",
        "    rag_retrieval_config=rag.RagRetrievalConfig(\n",
        "        top_k=10,  # Optional\n",
        "        filter=rag.Filter(\n",
        "            vector_distance_threshold=0.5,  # Optional\n",
        "        ),\n",
        "    ),\n",
        "    text=\"What is RAG and why it is helpful?\",\n",
        ")\n",
        "print(response)\n",
        "\n",
        "# Optional: The retrieved context can be passed to any SDK or model generation API to generate final results.\n",
        "# context = \" \".join([context.text for context in response.contexts.contexts]).replace(\"\\n\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79ea89661842"
      },
      "source": [
        "### Create RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0ebceac3d816"
      },
      "outputs": [],
      "source": [
        "# Create a tool for the RAG Corpus\n",
        "rag_retrieval_tool = Tool(\n",
        "    retrieval=Retrieval(\n",
        "        vertex_rag_store=VertexRagStore(\n",
        "            rag_corpora=[rag_corpus.name],\n",
        "            similarity_top_k=10,\n",
        "            vector_distance_threshold=0.5,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88fa7ede853"
      },
      "source": [
        "### Generate Content with Gemini using RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8dd928baecd4"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "124b36be8d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "b8f26b23-7b39-4e99-80f9-010055d3c708"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by enabling them to access and incorporate external data sources when generating responses. It combines information retrieval systems with the generative power of LLMs, allowing them to go beyond their pre-trained data and access up-to-date and specific information. When a user asks a question, the RAG system retrieves relevant information from external data sources and provides it to the LLM as additional context, which the LLM then uses to generate a more accurate and informative response. RAG is helpful because it allows access to up-to-date information, improves accuracy and factual grounding, enhances contextual relevance, increases trust and transparency, and is cost-efficient.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is RAG?\",\n",
        "    config=GenerateContentConfig(tools=[rag_retrieval_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0268fe43d41c"
      },
      "source": [
        "### Generate Content with Llama3 using RAG Retrieval Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f6e67ee7968c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9e8bb2-eaf4-4122-c3c9-30326c8a90ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ],
      "source": [
        "from vertexai import generative_models\n",
        "\n",
        "# Load tool into Llama model\n",
        "rag_retrieval_tool = generative_models.Tool.from_retrieval(\n",
        "    retrieval=rag.Retrieval(\n",
        "        source=rag.VertexRagStore(\n",
        "            rag_resources=[rag.RagResource(rag_corpus=rag_corpus.name)],\n",
        "            rag_retrieval_config=rag.RagRetrievalConfig(\n",
        "                top_k=10,  # Optional\n",
        "                filter=rag.Filter(\n",
        "                    vector_distance_threshold=0.5,  # Optional\n",
        "                ),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "llama_model = generative_models.GenerativeModel(\n",
        "    # your self-deployed endpoint for Llama3\n",
        "    \"projects/{project}/locations/{location}/endpoints/{endpoint_resource_id}\",\n",
        "    tools=[rag_retrieval_tool],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c6d710b6dece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "25fd308b-417f-46ca-a4ba-e1d616030a6b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unsupported region for Vertex AI, select from frozenset({'me-west1', 'us-west3', 'europe-west4', 'southamerica-east1', 'northamerica-northeast1', 'europe-west8', 'us-central1', 'asia-northeast3', 'asia-south1', 'europe-west1', 'us-west1', 'us-east4', 'europe-west9', 'europe-west12', 'global', 'asia-southeast2', 'me-central2', 'europe-southwest1', 'asia-east2', 'southamerica-west1', 'europe-west2', 'us-west4', 'europe-west3', 'us-east5', 'asia-southeast1', 'australia-southeast1', 'us-west2', 'us-east7', 'africa-south1', 'asia-northeast2', 'europe-north1', 'northamerica-northeast2', 'europe-central2', 'us-east1', 'asia-east1', 'europe-west6', 'australia-southeast2', 'me-central1', 'asia-northeast1', 'us-south1', 'asia-south2'})",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-524673977.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllama_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is RAG?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[0m\n\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             return self._generate_content(\n\u001b[0m\u001b[1;32m    711\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         )\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mgapic_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgapic_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/functools.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NOT_FOUND\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                 \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_prediction_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3417\u001b[0m                 \u001b[0;34m\"from vertexai.preview import generative_models\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m             )\n\u001b[0;32m-> 3419\u001b[0;31m         return aiplatform_initializer.global_config.create_client(\n\u001b[0m\u001b[1;32m   3420\u001b[0m             \u001b[0mclient_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_service_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPredictionServiceClient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3421\u001b[0m             \u001b[0mlocation_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_location\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/initializer.py\u001b[0m in \u001b[0;36mcreate_client\u001b[0;34m(self, client_class, credentials, location_override, prediction_client, api_base_path_override, api_key, api_path_override, appended_user_agent, appended_gapic_version)\u001b[0m\n\u001b[1;32m    602\u001b[0m         kwargs = {\n\u001b[1;32m    603\u001b[0m             \u001b[0;34m\"credentials\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \"client_options\": self.get_client_options(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mlocation_override\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation_override\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mprediction_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/initializer.py\u001b[0m in \u001b[0;36mget_client_options\u001b[0;34m(self, location_override, prediction_client, api_base_path_override, api_key, api_path_override)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             service_base_path = api_base_path_override or (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/utils/__init__.py\u001b[0m in \u001b[0;36mvalidate_region\u001b[0;34m(region)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mregion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregion\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUPPORTED_REGIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;34mf\"Unsupported region for Vertex AI, select from {constants.SUPPORTED_REGIONS}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Unsupported region for Vertex AI, select from frozenset({'me-west1', 'us-west3', 'europe-west4', 'southamerica-east1', 'northamerica-northeast1', 'europe-west8', 'us-central1', 'asia-northeast3', 'asia-south1', 'europe-west1', 'us-west1', 'us-east4', 'europe-west9', 'europe-west12', 'global', 'asia-southeast2', 'me-central2', 'europe-southwest1', 'asia-east2', 'southamerica-west1', 'europe-west2', 'us-west4', 'europe-west3', 'us-east5', 'asia-southeast1', 'australia-southeast1', 'us-west2', 'us-east7', 'africa-south1', 'asia-northeast2', 'europe-north1', 'northamerica-northeast2', 'europe-central2', 'us-east1', 'asia-east1', 'europe-west6', 'australia-southeast2', 'me-central1', 'asia-northeast1', 'us-south1', 'asia-south2'})"
          ]
        }
      ],
      "source": [
        "response = llama_model.generate_content(\"What is RAG?\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "intro_rag_engine.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}